{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3845,"status":"ok","timestamp":1623909044834,"user":{"displayName":"Nipun Paul","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12165927448334748533"},"user_tz":-360},"id":"wFieqEP6BuSx","outputId":"9d118f81-2ddd-462d-ad46-049f8ac038bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["NLTK Downloader\n","---------------------------------------------------------------------------\n","    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n","---------------------------------------------------------------------------\n","Downloader\u003e q\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import nltk\n","\n","nltk.download()"]},{"cell_type":"markdown","metadata":{"id":"RkBel8sgDIzH"},"source":["# Tokenizing words and Sentences"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":361,"status":"ok","timestamp":1623908952846,"user":{"displayName":"Nipun Paul","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12165927448334748533"},"user_tz":-360},"id":"KdbAier0Cfeb"},"outputs":[],"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","example_test=\"Hello Mr. Smith, how are you doing today? The weather is great and python is awesome. The sky is pinkish-blue. You should not eat cardboard.\"\n","# print(sent_tokenize(example_test))\n","# print(word_tokenize(example_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0ig56oMFrRf"},"outputs":[],"source":["for i in word_tokenize(example_test):\n","  print(i)"]},{"cell_type":"markdown","metadata":{"id":"dT0SY5zpJ1pN"},"source":["# Stop Words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1623853163782,"user":{"displayName":"Nipun Paul","photoUrl":"","userId":"12165927448334748533"},"user_tz":-360},"id":"ZnwJOXiiJ3Qn","outputId":"00ea59bb-a0ba-40cc-ce9c-2386760923d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'they', \"you'd\", 'haven', 'themselves', 'was', 'had', 'before', 'of', 'ourselves', 'so', 'them', 'if', \"isn't\", 'him', 'yourselves', 'now', \"won't\", 'couldn', 'in', 'yours', 'doing', 'from', 'mustn', 'ain', 'have', \"needn't\", 'below', 'both', 'd', 'wasn', \"hadn't\", 'each', 'only', 'myself', 'mightn', 'this', 'won', \"weren't\", 'having', 'll', 'during', 'here', 'by', 'it', 'why', 'once', \"shan't\", 'all', \"you'll\", 'few', \"that'll\", 'weren', 'which', \"shouldn't\", 'for', 'do', 'been', 'again', 'down', 'has', 'shouldn', 'isn', 'be', 'theirs', \"don't\", \"haven't\", 'up', 'me', 'your', 'to', 'am', 'but', 'there', \"she's\", 'will', 'his', 'an', 'being', 'we', \"wouldn't\", 'how', 'while', 'their', 'as', 'y', 'himself', 'into', 'until', 'you', 'against', \"hasn't\", 'and', 'with', 'not', 'i', 'shan', 'what', 'ma', \"doesn't\", 'any', 'nor', 'more', 'were', 'who', 'didn', 'no', 'are', 'at', 'our', 'm', 'aren', 'itself', \"you're\", 'wouldn', 'should', 'her', 'other', \"wasn't\", 'or', 'under', 'then', 's', \"should've\", 're', 'whom', \"you've\", 'on', 'when', 'where', 'the', 'over', 'herself', 'same', 'after', 'very', 'between', 'those', 'own', \"it's\", 'too', 'yourself', \"couldn't\", 'doesn', 'hasn', 'that', 'most', 've', 'ours', 'hers', 'he', 'such', 'can', \"mightn't\", 'does', 'some', \"aren't\", 'than', 'about', \"didn't\", 'off', 'these', 'needn', 'did', 'my', 'she', 'o', 'don', 'hadn', 'above', 'further', 'its', 'because', 'a', 't', 'just', 'is', \"mustn't\", 'through', 'out'}\n","['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"]}],"source":["from nltk.corpus import stopwords\n","\n","example_sentence=\"This is an example showing off stop word filtration.\"\n","# set of stop words\n","stop_words=set(stopwords.words('english'))\n","print(stop_words)\n","\n","words=word_tokenize(example_sentence)\n","filtered_sentence=[]\n","for w in words:\n","  if w not in stop_words:\n","    filtered_sentence.append(w)\n","\n","print(filtered_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":567,"status":"ok","timestamp":1623853255361,"user":{"displayName":"Nipun Paul","photoUrl":"","userId":"12165927448334748533"},"user_tz":-360},"id":"5_z5nXxDLn0H","outputId":"fffc883a-71b2-465f-9ce4-d6c55f0d1fc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["['This', 'example', 'showing', 'stop', 'word', 'filtration', '.']\n"]}],"source":["filtered_sentence=[w for w in words if not w in stop_words]\n","print(filtered_sentence)"]},{"cell_type":"markdown","metadata":{"id":"hnFEaQKSNUAC"},"source":["# Stemming"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":555,"status":"ok","timestamp":1623853686036,"user":{"displayName":"Nipun Paul","photoUrl":"","userId":"12165927448334748533"},"user_tz":-360},"id":"23dQbDsbNVG5","outputId":"c8440ea6-6b60-46f9-aceb-c76f4c6547d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["python\n","python\n","python\n","python\n","pythonli\n"]}],"source":["from nltk.stem import PorterStemmer\n","\n","ps=PorterStemmer()\n","\n","example_words=[\"Python\", \"Pythoner\", \"Pythoning\", \"Pythoned\", \"Pythonly\"]\n","\n","for w in example_words:\n","  print(ps.stem(w))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jevf0omYONfK"},"outputs":[],"source":["new_text=\"It is very import to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n","\n","words=word_tokenize(new_text)\n","for w in words:\n","  print(ps.stem(w))"]},{"cell_type":"markdown","metadata":{"id":"coJPOR1RPFVq"},"source":["# Part of Speech Tagging"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_dFmHMwPQaD"},"outputs":[],"source":["from nltk.corpus import state_union\n","from nltk.tokenize import PunktSentenceTokenizer\n","\n","train_text=state_union.raw('2005-GWBush.txt')\n","sample_text=state_union.raw('2006-GWBush.txt')\n","\n","custom_sent_tokenizer=PunktSentenceTokenizer(train_text)\n","tokenized=custom_sent_tokenizer.tokenize(sample_text)\n","\n","def process_content():\n","  try:\n","    for i in tokenized:\n","      words=nltk.word_tokenize(i)\n","      tagged=nltk.pos_tag(words)\n","\n","      print(tagged)\n","  except Exception as e:\n","    print(str(e))\n","\n","process_content()"]},{"cell_type":"markdown","metadata":{"id":"WLGZ-iOgSgQS"},"source":["# Chunking"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"i1lfSip9Shtg"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n","0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ghostscript is already the newest version (9.26~dfsg+0-0ubuntu0.18.04.14).\n","python3-tk is already the newest version (3.6.9-1~18.04).\n","0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"]}],"source":["from nltk.corpus import state_union\n","from nltk.tokenize import PunktSentenceTokenizer\n","### CREATE VIRTUAL DISPLAY ###\n","!apt-get install -y xvfb # Install X Virtual Frame Buffer\n","import os\n","os.system('Xvfb :1 -screen 0 1600x1200x16  \u0026')    # create virtual display with size 1600x1200 and 16 bit color. Color can be changed to 24 or 8\n","os.environ['DISPLAY']=':1.0'\n","%matplotlib inline\n","### INSTALL GHOSTSCRIPT (Required to display NLTK trees) ###\n","!apt install ghostscript python3-tk\n","from nltk.tree import Tree\n","from IPython.display import display\n","\n","train_text=state_union.raw('2005-GWBush.txt')\n","sample_text=state_union.raw('2006-GWBush.txt')\n","\n","custom_sent_tokenizer=PunktSentenceTokenizer(train_text)\n","tokenized=custom_sent_tokenizer.tokenize(sample_text)\n","\n","def process_content():\n","  try:\n","    for i in tokenized:\n","      words=nltk.word_tokenize(i)\n","      tagged=nltk.pos_tag(words)\n","\n","      chunkGram=r'''Chunk: {\u003cRB.?\u003e*\u003cVB.?\u003e*\u003cNNP\u003e\u003cNN\u003e?}'''\n","\n","      chunkParser=nltk.RegexpParser(chunkGram)\n","      chunked=chunkParser.parse(tagged)\n","\n","      chunked.draw()\n","  except Exception as e:\n","    print(str(e))\n","\n","process_content()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7iEiQIXfdSa"},"outputs":[],"source":[""]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMq/9uRTIz4ipmAlQPAWAo6","name":"NLP.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}